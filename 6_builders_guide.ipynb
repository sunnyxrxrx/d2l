{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddd4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fbbbbc",
   "metadata": {},
   "source": [
    "## neuron->layer->module(block)->model\n",
    "\n",
    "每个layer/module必须提供：\n",
    "\n",
    "- 将输入数据作为其前向传播函数的参数。\n",
    "- 通过前向传播函数来生成输出。\n",
    "- 计算其输出关于输入的梯度，可通过其反向传播函数进行访问。\n",
    "- 存储和访问前向传播计算所需的参数。\n",
    "- 根据需要初始化模型参数。\n",
    "\n",
    "如果我们组合已有的nn提供的layer，我们只需要实现前向传播\n",
    "\n",
    "layer/module一般实现成nn.Module的子类。对于一个nn.Module对象net，我们只需要对它调用net(X)，相当于执行了forward函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "253f7448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#一个自定义layer\n",
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c59660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544085ed",
   "metadata": {},
   "source": [
    "nn.Sequential可以连接layers，也可以连接modules组成更大的modules，返回一个nn.Module类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "783849b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequential作为一个子module\n",
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5efc78fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0869, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2, 20)\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a364654",
   "metadata": {},
   "source": [
    "## 参数访问\n",
    "- ``state_dict``方法可以访问nn.Module类的所有参数\n",
    "- ``parameter()``方法会生成一个访问nn.Module类所有参数的迭代器\n",
    "- 当通过Sequential类定义模型时， 我们可以通过索引来访问模型的任意层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bc95d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[-0.0292,  0.0929, -0.2302, -0.0729,  0.1692, -0.1976,  0.0834, -0.1896,\n",
      "         -0.0728, -0.2421,  0.0893,  0.1170,  0.0648,  0.0758, -0.1985,  0.1306],\n",
      "        [ 0.1271, -0.0399, -0.0378,  0.0349, -0.0319, -0.2491,  0.1688, -0.2114,\n",
      "          0.0871,  0.0473, -0.0180,  0.0832,  0.2257,  0.0589,  0.2080,  0.1223],\n",
      "        [ 0.2117, -0.1851,  0.1861,  0.1620, -0.0733,  0.2227,  0.0680, -0.2488,\n",
      "         -0.1128,  0.0395,  0.0548, -0.1617, -0.0193,  0.1155,  0.0379,  0.1138],\n",
      "        [-0.0965,  0.2233,  0.1100, -0.0832,  0.1523,  0.0557,  0.1497,  0.1412,\n",
      "         -0.0956,  0.2037, -0.0574,  0.0406, -0.1458, -0.1391,  0.0863, -0.2172],\n",
      "        [-0.1007, -0.2260,  0.1123, -0.2036, -0.1287, -0.1272, -0.0531, -0.0556,\n",
      "          0.1041,  0.1706, -0.1618, -0.1017, -0.1912, -0.0980,  0.1026, -0.0382],\n",
      "        [ 0.0472, -0.1707,  0.1211, -0.1312,  0.1688, -0.0168,  0.2379, -0.1413,\n",
      "          0.2223,  0.2262, -0.2076, -0.0488, -0.1225, -0.1564,  0.0976,  0.0236],\n",
      "        [ 0.2182,  0.1006,  0.0561, -0.1962,  0.0023, -0.2156, -0.1041,  0.2431,\n",
      "         -0.1588, -0.1686,  0.1886,  0.2445, -0.2381,  0.1758,  0.0956,  0.2342],\n",
      "        [-0.1665, -0.0918,  0.1208, -0.1641, -0.0721,  0.1054, -0.1684, -0.0374,\n",
      "         -0.0486,  0.0952, -0.0679,  0.1675, -0.0806, -0.0769,  0.1993, -0.1496],\n",
      "        [-0.1845,  0.2015, -0.0504,  0.0159, -0.1680, -0.1988,  0.2246, -0.0558,\n",
      "         -0.1936, -0.0296,  0.1420,  0.2252, -0.0655,  0.0244,  0.1084,  0.2258],\n",
      "        [-0.2386, -0.2033,  0.1015, -0.0154,  0.1530,  0.2014, -0.2275,  0.1323,\n",
      "         -0.1937,  0.1539,  0.1085, -0.1004,  0.0629,  0.2302, -0.0855,  0.0204],\n",
      "        [-0.2154, -0.0512,  0.2166, -0.1373,  0.0983, -0.0085,  0.1690, -0.0304,\n",
      "         -0.1054,  0.2212,  0.0825,  0.0982,  0.2331,  0.1968, -0.2090, -0.1713],\n",
      "        [ 0.0091, -0.1617, -0.1762, -0.1913, -0.1688,  0.2258, -0.1936, -0.1210,\n",
      "          0.0545,  0.0580,  0.1445,  0.0018, -0.0158,  0.0821, -0.1168,  0.1451],\n",
      "        [-0.2454, -0.1380, -0.0617, -0.0302, -0.0074, -0.0476, -0.1259, -0.0574,\n",
      "          0.1869, -0.1148, -0.1192, -0.1927,  0.0655, -0.0105, -0.1427, -0.1970],\n",
      "        [-0.2269, -0.2117,  0.2155,  0.0266, -0.0382,  0.0411, -0.1415,  0.0013,\n",
      "         -0.0768,  0.0522, -0.0290,  0.1004, -0.2277, -0.2312,  0.1432, -0.2337],\n",
      "        [-0.2385,  0.1787,  0.1954, -0.2048,  0.1341, -0.2362,  0.1004,  0.1684,\n",
      "          0.1382,  0.0266,  0.1088,  0.1561,  0.1436,  0.1047, -0.2455,  0.0963],\n",
      "        [ 0.1036, -0.1512, -0.1913,  0.0776, -0.2322, -0.1957,  0.1917,  0.0566,\n",
      "          0.1241, -0.1913,  0.0922,  0.1509, -0.1696,  0.1155, -0.1613,  0.1159],\n",
      "        [ 0.0377,  0.1574, -0.1754,  0.0036,  0.0868,  0.0143,  0.1169,  0.1658,\n",
      "          0.1286,  0.1140, -0.0991, -0.1336,  0.0680, -0.0073, -0.1463, -0.0609],\n",
      "        [ 0.0014,  0.0935, -0.0566, -0.1478,  0.0250, -0.2159, -0.1284, -0.0640,\n",
      "          0.1268, -0.2360, -0.1855,  0.0426, -0.2018,  0.1727,  0.1830, -0.0919],\n",
      "        [-0.0426, -0.1062, -0.1175, -0.1967,  0.1989,  0.1272,  0.1075,  0.0723,\n",
      "          0.1368, -0.0767, -0.2361,  0.2185, -0.1642,  0.1065,  0.1749,  0.1571],\n",
      "        [-0.2084, -0.2177, -0.1348, -0.1532,  0.0748,  0.1439, -0.1723, -0.0758,\n",
      "          0.1935, -0.0014,  0.2190,  0.2372, -0.0491,  0.1027, -0.1901, -0.2462]])), ('bias', tensor([-0.1367,  0.0477,  0.0889,  0.2198,  0.0500, -0.1709,  0.2299, -0.0590,\n",
      "         0.1442, -0.1662,  0.1817,  0.0371,  0.1362,  0.2248, -0.1546, -0.1655,\n",
      "        -0.2050, -0.2243, -0.0849, -0.0237]))])\n"
     ]
    }
   ],
   "source": [
    "print(chimera[1].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91b5a8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([64, 20])\n",
      "True torch.Size([64])\n",
      "True torch.Size([32, 64])\n",
      "True torch.Size([32])\n",
      "True torch.Size([16, 32])\n",
      "True torch.Size([16])\n",
      "True torch.Size([20, 16])\n",
      "True torch.Size([20])\n",
      "True torch.Size([20, 20])\n",
      "True torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "for i in chimera.parameters():\n",
    "    print (i.requires_grad,i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf6743f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.0172,  0.1755, -0.0482,  0.1067, -0.0745,  0.1671, -0.2355, -0.2153,\n",
      "        -0.0984, -0.1302, -0.0109, -0.0282, -0.1354, -0.2063, -0.1290,  0.0137,\n",
      "         0.1248,  0.2119,  0.1369,  0.2051], requires_grad=True)\n",
      "tensor([-0.0172,  0.1755, -0.0482,  0.1067, -0.0745,  0.1671, -0.2355, -0.2153,\n",
      "        -0.0984, -0.1302, -0.0109, -0.0282, -0.1354, -0.2063, -0.1290,  0.0137,\n",
      "         0.1248,  0.2119,  0.1369,  0.2051])\n",
      "tensor([-0.0439,  0.0461, -0.1094, -0.0604,  0.0629, -0.1719,  0.0335, -0.0089,\n",
      "         0.1701,  0.0710,  0.1843,  0.1261,  0.0046,  0.2213,  0.0181,  0.1970,\n",
      "         0.0012,  0.1993,  0.0825, -0.2197])\n"
     ]
    }
   ],
   "source": [
    "print(type(chimera[1].bias))\n",
    "print(chimera[1].bias)\n",
    "print(chimera[1].bias.data)\n",
    "#对于自定义层，我们直接访问数据成员\n",
    "print(chimera[2].linear.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794f7263",
   "metadata": {},
   "source": [
    "## 自定义初始化\n",
    "这里``apply``方法会深度优先遍历每一个模块(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f4875e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([64, 20])\n",
      "Init weight torch.Size([32, 64])\n",
      "Init weight torch.Size([16, 32])\n",
      "Init weight torch.Size([20, 16])\n",
      "Init weight torch.Size([20, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -8.8114,  5.6600, -0.0000, -9.6173, -0.0000, -0.0000,  7.6698,\n",
       "          6.4698,  0.0000, -9.2073, -0.0000, -0.0000,  0.0000,  0.0000,  5.9374],\n",
       "        [ 0.0000, -0.0000, -5.3784,  0.0000,  0.0000, -9.8710, -0.0000, -0.0000,\n",
       "         -0.0000, -6.8422,  0.0000, -6.8476, -6.7283, -6.1795, -5.9927, -8.6711]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "chimera.apply(my_init)\n",
    "chimera[1].weight[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d4f45b",
   "metadata": {},
   "source": [
    "## 参数共享\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1483808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "Xx=torch.rand(4)\n",
    "net(Xx)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e12721",
   "metadata": {},
   "source": [
    "## 延后初始化 lazy initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e274bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UninitializedParameter>\n",
      "Parameter containing:\n",
      "tensor([[-0.1950, -0.1744, -0.0641,  ..., -0.0823, -0.1154, -0.0948],\n",
      "        [-0.0350,  0.0605, -0.1897,  ...,  0.0568, -0.0886,  0.0389],\n",
      "        [ 0.1849, -0.1263,  0.0249,  ...,  0.2083, -0.0185, -0.1320],\n",
      "        ...,\n",
      "        [-0.0455,  0.1780, -0.1234,  ...,  0.1639, -0.1975, -0.1750],\n",
      "        [ 0.1486,  0.1205,  0.1950,  ..., -0.0156, -0.0652,  0.1930],\n",
      "        [-0.0490, -0.0500,  0.1785,  ...,  0.0401, -0.1717, -0.0313]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))\n",
    "print(net[0].weight)  # 尚未初始化\n",
    "X = torch.rand(2, 20)\n",
    "net(X)\n",
    "print(net[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a5f1ed",
   "metadata": {},
   "source": [
    "## 读写文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b012b798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\a_university\\1.3MLDL\\LMDL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "476d48b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.arange(4)\n",
    "torch.save(x,'test_for_saving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92f8d549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2=torch.load('test_for_saving')\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5c79d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9876cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)\n",
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d409dc2",
   "metadata": {},
   "source": [
    "加载参数用``load_state_dict``方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a12c0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()\n",
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2lcuda",
   "language": "python",
   "name": "d2lcuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
